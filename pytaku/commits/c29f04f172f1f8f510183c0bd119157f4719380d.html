<!DOCTYPE html>
<html lang="en" style="font-family: monospace;"><head><title>[c29f04f172] put the meat of all commands into a single module | pytaku | Boast</title><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"></head><body><strong><a href="../../">Repos</a> / <a href="../">pytaku</a> / c29f04f172</strong><hr><pre>commit c29f04f172f1f8f510183c0bd119157f4719380d
Author: Bùi Thành Nhân &lt;hi@imnhan.com&gt;
Date:   Sun May 24 22:45:20 2020 +0700

    put the meat of all commands into a single module

diff --git a/src/pytaku_scraper/commands.py b/src/pytaku_scraper/commands.py
new file mode 100644
index 0000000..b992209
--- /dev/null
+++ b/src/pytaku_scraper/commands.py
@@ -0,0 +1,48 @@
+import requests
+from django.db import transaction
+from django.utils import timezone
+
+from .models import DownloadResult, TaskQueue
+from .sites.mangadex import get_latest_id
+
+
+def put_download_tasks():
+    latest_id = get_latest_id()
+    print(&quot;Found latest title id:&quot;, latest_id)
+
+    result = TaskQueue.put_bulk(
+        &quot;download&quot;,
+        [
+            {&quot;url&quot;: f&quot;https://mangadex.org/api/?type=manga&amp;id={i}&quot;}
+            for i in range(1, latest_id + 1)
+        ],
+    )
+    print(f&#39;Successfully put {len(result)} &quot;download&quot; tasks.&#39;)
+
+
+def download_worker():
+    while True:
+        with transaction.atomic():
+            task = TaskQueue.pop(&quot;download&quot;)
+            task_id = task.id
+            print(f&quot;Processing task {task_id}: {task.payload}&quot;)
+            resp = requests.get(task.payload[&quot;url&quot;], timeout=30)
+            assert resp.status_code in (200, 404), f&quot;Unexpected error: {resp.text}&quot;
+
+            DownloadResult.objects.update_or_create(
+                url=task.payload[&quot;url&quot;],
+                method=&quot;get&quot;,
+                defaults={
+                    &quot;downloaded_at&quot;: timezone.now(),
+                    &quot;resp_body&quot;: resp.text,
+                    &quot;resp_status&quot;: resp.status_code,
+                },
+            )
+
+            task.finish()
+            print(&quot;Done task&quot;, task_id)
+
+
+def purge_queue(task_name):
+    count, _ = TaskQueue.objects.filter(name=task_name).delete()
+    print(f&#39;Deleted {count} &quot;{task_name}&quot; tasks.&#39;)
diff --git a/src/pytaku_scraper/management/commands/download_worker.py b/src/pytaku_scraper/management/commands/download_worker.py
new file mode 100644
index 0000000..87f3f75
--- /dev/null
+++ b/src/pytaku_scraper/management/commands/download_worker.py
@@ -0,0 +1,10 @@
+from django.core.management.base import BaseCommand
+
+from pytaku_scraper.commands import download_worker
+
+
+class Command(BaseCommand):
+    help = &quot;Download worker. Run as many as needed.&quot;
+
+    def handle(self, *args, **options):
+        download_worker()
diff --git a/src/pytaku_scraper/management/commands/purge_queue.py b/src/pytaku_scraper/management/commands/purge_queue.py
index fe66b48..e21f547 100644
--- a/src/pytaku_scraper/management/commands/purge_queue.py
+++ b/src/pytaku_scraper/management/commands/purge_queue.py
@@ -1,17 +1,14 @@
 from django.core.management.base import BaseCommand
 
-from pytaku_scraper.models import TaskQueue
+from pytaku_scraper.commands import purge_queue
 
 
 class Command(BaseCommand):
     help = &quot;Delete all tasks in a queue.&quot;
 
     def add_arguments(self, parser):
-        parser.add_argument(&quot;task&quot;, choices=[&quot;scrape&quot;])
+        parser.add_argument(&quot;task&quot;)
 
     def handle(self, *args, **options):
         task = options[&quot;task&quot;]
-        assert task == &quot;scrape&quot;
-
-        count, _ = TaskQueue.objects.filter(name=task).delete()
-        print(f&#39;Deleted {count} &quot;{task}&quot; tasks.&#39;)
+        purge_queue(task)
diff --git a/src/pytaku_scraper/management/commands/put_download_tasks.py b/src/pytaku_scraper/management/commands/put_download_tasks.py
new file mode 100644
index 0000000..689477f
--- /dev/null
+++ b/src/pytaku_scraper/management/commands/put_download_tasks.py
@@ -0,0 +1,10 @@
+from django.core.management.base import BaseCommand
+
+from pytaku_scraper.commands import put_download_tasks
+
+
+class Command(BaseCommand):
+    help = &quot;Puts download tasks for mangadex titles.&quot;
+
+    def handle(self, *args, **options):
+        put_download_tasks()
diff --git a/src/pytaku_scraper/management/commands/put_tasks.py b/src/pytaku_scraper/management/commands/put_tasks.py
deleted file mode 100644
index 5e85e94..0000000
--- a/src/pytaku_scraper/management/commands/put_tasks.py
+++ /dev/null
@@ -1,25 +0,0 @@
-from django.core.management.base import BaseCommand
-
-from pytaku_scraper.models import TaskQueue
-
-
-class Command(BaseCommand):
-    help = &quot;Puts various tasks.&quot;
-
-    def add_arguments(self, parser):
-        parser.add_argument(&quot;task&quot;, choices=[&quot;scrape&quot;])
-        parser.add_argument(&quot;start_id&quot;, type=int)
-        parser.add_argument(&quot;end_id&quot;, type=int)
-
-    def handle(self, *args, **options):
-        assert options[&quot;task&quot;] == &quot;scrape&quot;
-
-        result = TaskQueue.put_bulk(
-            &quot;scrape&quot;,
-            [
-                {&quot;url&quot;: f&quot;https://mangadex.org/api/?type=manga&amp;id={i}&quot;}
-                for i in range(options[&quot;start_id&quot;], options[&quot;end_id&quot;] + 1)
-            ],
-        )
-
-        print(&quot;Result:&quot;, result)
diff --git a/src/pytaku_scraper/management/commands/scrape.py b/src/pytaku_scraper/management/commands/scrape.py
deleted file mode 100644
index c1fe0cf..0000000
--- a/src/pytaku_scraper/management/commands/scrape.py
+++ /dev/null
@@ -1,30 +0,0 @@
-import requests
-from django.core.management.base import BaseCommand
-from django.db import transaction
-
-from pytaku_scraper.models import ScrapeAttempt, TaskQueue
-
-
-class Command(BaseCommand):
-    help = &quot;Scrape worker. Run as many as needed.&quot;
-
-    def handle(self, *args, **options):
-        task_name = &quot;scrape&quot;
-
-        while True:
-            with transaction.atomic():
-                task = TaskQueue.pop(task_name)
-                task_id = task.id
-                print(f&quot;Processing task {task_id}: {task.payload}&quot;)
-                resp = requests.get(task.payload[&quot;url&quot;], timeout=30)
-                assert resp.status_code in (200, 404), f&quot;Unexpected error: {resp.text}&quot;
-
-                ScrapeAttempt.objects.create(
-                    url=task.payload[&quot;url&quot;],
-                    method=&quot;get&quot;,  # TODO
-                    resp_body=resp.text,
-                    resp_status=resp.status_code,
-                )
-
-                task.finish()
-                print(&quot;Done task&quot;, task_id)
diff --git a/src/pytaku_scraper/migrations/0002_auto_20200524_1413.py b/src/pytaku_scraper/migrations/0002_auto_20200524_1413.py
new file mode 100644
index 0000000..181b284
--- /dev/null
+++ b/src/pytaku_scraper/migrations/0002_auto_20200524_1413.py
@@ -0,0 +1,34 @@
+# Generated by Django 3.0.5 on 2020-05-24 14:13
+
+from django.db import migrations, models
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        (&#39;pytaku_scraper&#39;, &#39;0001_initial&#39;),
+    ]
+
+    operations = [
+        migrations.CreateModel(
+            name=&#39;DownloadResult&#39;,
+            fields=[
+                (&#39;id&#39;, models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=&#39;ID&#39;)),
+                (&#39;created_at&#39;, models.DateTimeField(auto_now_add=True)),
+                (&#39;url&#39;, models.CharField(max_length=1024)),
+                (&#39;method&#39;, models.CharField(default=&#39;get&#39;, max_length=7)),
+                (&#39;resp_body&#39;, models.TextField()),
+                (&#39;resp_status&#39;, models.IntegerField()),
+            ],
+            options={
+                &#39;db_table&#39;: &#39;download_result&#39;,
+            },
+        ),
+        migrations.DeleteModel(
+            name=&#39;ScrapeAttempt&#39;,
+        ),
+        migrations.AddConstraint(
+            model_name=&#39;downloadresult&#39;,
+            constraint=models.UniqueConstraint(fields=(&#39;url&#39;, &#39;method&#39;), name=&#39;unique_url_method&#39;),
+        ),
+    ]
diff --git a/src/pytaku_scraper/migrations/0003_auto_20200524_1508.py b/src/pytaku_scraper/migrations/0003_auto_20200524_1508.py
new file mode 100644
index 0000000..66b7737
--- /dev/null
+++ b/src/pytaku_scraper/migrations/0003_auto_20200524_1508.py
@@ -0,0 +1,22 @@
+# Generated by Django 3.0.5 on 2020-05-24 15:08
+
+from django.db import migrations, models
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        (&#39;pytaku_scraper&#39;, &#39;0002_auto_20200524_1413&#39;),
+    ]
+
+    operations = [
+        migrations.AlterField(
+            model_name=&#39;taskqueue&#39;,
+            name=&#39;name&#39;,
+            field=models.CharField(choices=[(&#39;Download&#39;, &#39;download&#39;)], max_length=100),
+        ),
+        migrations.AddConstraint(
+            model_name=&#39;taskqueue&#39;,
+            constraint=models.UniqueConstraint(fields=(&#39;name&#39;, &#39;payload&#39;), name=&#39;unique_url_payload&#39;),
+        ),
+    ]
diff --git a/src/pytaku_scraper/migrations/0004_auto_20200524_1538.py b/src/pytaku_scraper/migrations/0004_auto_20200524_1538.py
new file mode 100644
index 0000000..1a70a30
--- /dev/null
+++ b/src/pytaku_scraper/migrations/0004_auto_20200524_1538.py
@@ -0,0 +1,18 @@
+# Generated by Django 3.0.5 on 2020-05-24 15:38
+
+from django.db import migrations
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        (&#39;pytaku_scraper&#39;, &#39;0003_auto_20200524_1508&#39;),
+    ]
+
+    operations = [
+        migrations.RenameField(
+            model_name=&#39;downloadresult&#39;,
+            old_name=&#39;created_at&#39;,
+            new_name=&#39;downloaded_at&#39;,
+        ),
+    ]
diff --git a/src/pytaku_scraper/models.py b/src/pytaku_scraper/models.py
index 0ff5391..4ac7710 100644
--- a/src/pytaku_scraper/models.py
+++ b/src/pytaku_scraper/models.py
@@ -1,7 +1,7 @@
 from django.contrib.postgres.fields import JSONField
 from django.db import models
 
-QUEUE_NAMES = [(&quot;Scrape&quot;, &quot;scrape&quot;)]
+QUEUE_NAMES = [(&quot;Download&quot;, &quot;download&quot;)]
 
 
 class TaskQueue(models.Model):
@@ -22,6 +22,11 @@ class TaskQueue(models.Model):
 
     class Meta:
         db_table = &quot;task_queue&quot;
+        constraints = [
+            models.UniqueConstraint(
+                fields=[&quot;name&quot;, &quot;payload&quot;], name=&quot;unique_url_payload&quot;
+            )
+        ]
 
     created_at = models.DateTimeField(auto_now_add=True)
     name = models.CharField(max_length=100, choices=QUEUE_NAMES)
@@ -34,7 +39,8 @@ def put(cls, name, payload):
     @classmethod
     def put_bulk(cls, name, payloads):
         return cls.objects.bulk_create(
-            [cls(name=name, payload=payload) for payload in payloads]
+            [cls(name=name, payload=payload) for payload in payloads],
+            ignore_conflicts=True,
         )
 
     @classmethod
@@ -56,16 +62,17 @@ def finish(self):
         return self.delete()
 
 
-class ScrapeAttempt(models.Model):
+class DownloadResult(models.Model):
     class Meta:
-        db_table = &quot;scrape_attempt&quot;
+        db_table = &quot;download_result&quot;
+        constraints = [
+            models.UniqueConstraint(fields=[&quot;url&quot;, &quot;method&quot;], name=&quot;unique_url_method&quot;)
+        ]
 
-    scraped_at = models.DateTimeField(auto_now_add=True)
+    downloaded_at = models.DateTimeField(auto_now_add=True)
 
     url = models.CharField(max_length=1024)
-    method = models.CharField(max_length=7)
-    headers = JSONField(default=dict)
-    body = models.TextField()
+    method = models.CharField(max_length=7, default=&quot;get&quot;)
 
     resp_body = models.TextField()
     resp_status = models.IntegerField()
diff --git a/src/pytaku_scraper/sites/mangadex.py b/src/pytaku_scraper/sites/mangadex.py
index 189c440..8ce7558 100644
--- a/src/pytaku_scraper/sites/mangadex.py
+++ b/src/pytaku_scraper/sites/mangadex.py
@@ -60,7 +60,6 @@ def scrape_title(original_id):
     source_url = title_url_from_id(original_id)
     html = session.get(source_url).text
     soup = BeautifulSoup(html, &quot;lxml&quot;)
-    print(soup)
 
     url = soup.select(&#39;link[rel=&quot;canonical&quot;]&#39;)[0].attrs[&quot;href&quot;]
     name = soup.select(&quot;.card-header span.mx-1&quot;)[0].text
@@ -180,3 +179,12 @@ def scrape_chapter(original_id):
         &quot;name&quot;: data[&quot;title&quot;],
         &quot;pages&quot;: pages,
     }
+
+
+def get_latest_id():
+    resp = session.get(&quot;https://mangadex.org/&quot;)
+    assert resp.status_code == 200, resp.text
+    soup = BeautifulSoup(resp.text, &quot;lxml&quot;)
+    latest_href = soup.select_one(&quot;#new_titles_owl_carousel a&quot;).attrs[&quot;href&quot;]
+    latest_id = re.search(r&quot;/(\d+)/&quot;, latest_href).group(1)
+    return int(latest_id)
</pre></body></html>